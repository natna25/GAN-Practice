{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Conditional GAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Mx1U5T1AiBLQ","colab_type":"text"},"source":["# Conditional GAN (cGAN)\n","\n","This notebook follows the tutorial from the website machine learning mastery:\n","\n","https://machinelearningmastery.com/how-to-develop-a-conditional-generative-adversarial-network-from-scratch/\n","\n","This kind of GAN can be conditioned on the input image class which allows for a generator that can generate samples from each class by conditioning the generation. This both allows for more flexibility in the training process and adds more options for generation than simply sampling random points from the latent space. The implementation here follows from the original cGAN paper where the image labels are added as a one-hot-encoded vector that is combined with the input in both the discriminator and generator and then emebeded along with the other features, this has the effect of conditioning either the generator or discriminator on the class label.\n","\n","original paper:\n","https://arxiv.org/pdf/1411.1784.pdf\n"]},{"cell_type":"markdown","metadata":{"id":"QiH-BaR2ohgT","colab_type":"text"},"source":["## Loading the data\n","We will be using the CIFAR10 dataset again here. we can download it using keras\n"]},{"cell_type":"code","metadata":{"id":"XADJ1OVTh7Ed","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.datasets.cifar10 import load_data\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7G-bR4SypVcU","colab_type":"code","colab":{}},"source":["(x_train,y_train),(X_test,y_test) = load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nCSmvy-pjq_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SSChXZd0rzCv","colab_type":"text"},"source":["## Making the discriminator\n","\n","Since we are building a conditional gan, we will need two inputs, one for the image itself and another for the label vector that we will use to condition the generator and discriminator. \n","\n","Usually we would have used the Sequential API in order to build our model but we can't do that here as we have two different input which we won't merge immediatly. We first need to pass the label vector through an embeding layer before adding it to the convolutions down the line\n","\n","This here describes how to use the Funtional API which we will make use of for building our two models:\n","\n","https://machinelearningmastery.com/keras-functional-api-deep-learning/\n","\n"]},{"cell_type":"code","metadata":{"id":"kcm6Bu5wtBA3","colab_type":"code","colab":{}},"source":["from keras.models import Model\n","from keras.layers import Input\n","from keras.layers import Conv2D\n","from keras.layers import LeakyReLU\n","from keras.layers import Reshape\n","from keras.layers import Dense\n","from keras.layers import Embedding\n","from keras.layers import Concatenate\n","from keras.layers import Dropout\n","from keras.layers import Flatten\n","from keras.layers import Conv2DTranspose\n","from keras.optimizers import Adam\n","\n","\n","\n","# define the standalone discriminator model\n","def make_discriminator(in_shape=(32,32,3), n_classes=10):\n","    \n","    # label input\n","    in_label = Input(shape=(1,))\n","    # embedding for categorical input\n","    li = Embedding(n_classes, 50)(in_label)\n","    # scale up to image dimensions with linear activation\n","    n_nodes = in_shape[0] * in_shape[1] * 3\n","    li = Dense(n_nodes)(li)\n","    # reshape to additional channel\n","    li = Reshape((in_shape[0], in_shape[1], 3))(li)\n","    # image input\n","    in_image = Input(shape=in_shape)\n","    # concat label as a channel\n","    merge = Concatenate()([in_image, li])\n","    # downsample\n","    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","    # downsample\n","    fe = Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n","    fe = LeakyReLU(alpha=0.2)(fe)\n","    # flatten feature maps\n","    fe = Flatten()(fe)\n","    # dropout\n","    fe = Dropout(0.4)(fe)\n","    # output\n","    out_layer = Dense(1, activation='sigmoid')(fe)\n","    # define model\n","    model = Model([in_image, in_label], out_layer)\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    return model\n","\n","\n","def define_generator(latent_dim=100, n_classes=10):\n","    # label input\n","    in_label = Input(shape=(1,))\n","    # embedding for categorical input\n","    li = Embedding(n_classes, 50)(in_label)\n","    # linear multiplication\n","    n_nodes =  8 * 8 * 3\n","    li = Dense(n_nodes)(li)\n","    # reshape to additional channel\n","    li = Reshape((8, 8, 3))(li)\n","    # image generator input\n","    in_lat = Input(shape=(latent_dim,))\n","    # foundation for 8x8 image\n","    n_nodes = 128 * 8 * 8\n","    gen = Dense(n_nodes)(in_lat)\n","    gen = LeakyReLU(alpha=0.2)(gen)\n","    gen = Reshape((8, 8, 128))(gen)\n","    # merge image gen and label input\n","    merge = Concatenate()([gen, li])\n","    # upsample to 14x14\n","    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n","    gen = LeakyReLU(alpha=0.2)(gen)\n","    # upsample to 28x28\n","    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n","    gen = LeakyReLU(alpha=0.2)(gen)\n","    # output\n","    out_layer = Conv2D(3, (3,3), activation='tanh', padding='same')(gen)\n","    # define model\n","    model = Model([in_lat, in_label], out_layer)\n"," \n","    return model\n","\n","# define the combined generator and discriminator model, for updating the generator\n","def define_gan(g_model, d_model):\n","    # make weights in the discriminator not trainable\n","    d_model.trainable = False\n","    # get noise and label inputs from generator model\n","    print(g_model.input)\n","    print(d_model.input)\n","    \n","    gen_noise, gen_label = g_model.input\n","    # get image output from the generator model\n","    gen_output = g_model.output\n","    # connect image output and label input from generator as inputs to discriminator\n","    gan_output = d_model([gen_output, gen_label])\n","    # define gan model as taking noise and label and outputting a classification\n","    model = Model([gen_noise, gen_label], gan_output)\n","    # compile model\n","    opt = Adam(lr=0.0002, beta_1=0.5)\n","    model.compile(loss='binary_crossentropy', optimizer=opt)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTXkern2EHBU","colab_type":"code","colab":{}},"source":["# load fashion mnist images\n","def load_real_samples():\n","    # load dataset\n","    (trainX, trainy), (_, _) = load_data()\n","    # expand to 3d, e.g. add channels\n","    X = trainX\n","    # convert from ints to floats\n","    X = X.astype('float32')\n","    # scale from [0,255] to [-1,1]\n","    X = (X - 127.5) / 127.5\n","    return [X, trainy]\n"," \n","# select real samples\n","def generate_real_samples(dataset, n_samples):\n","    # split into images and labels\n","    images, labels = dataset\n","    # choose random instances\n","    ix = np.random.randint(0, images.shape[0], n_samples)\n","    # select images and labels\n","    X, labels = images[ix], labels[ix]\n","    # generate class labels\n","    y = np.ones((n_samples, 1))\n","    return [X, labels], y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K1MQZpUJEN6f","colab_type":"code","colab":{}},"source":["def generate_latent_points(latent_dim, n_samples, n_classes=10):\n","\t# generate points in the latent space\n","\tx_input = np.random.randn(latent_dim * n_samples)\n","\t# reshape into a batch of inputs for the network\n","\tz_input = x_input.reshape(n_samples, latent_dim)\n","\t# generate labels\n","\tlabels = np.random.randint(0, n_classes, n_samples)\n","\treturn [z_input, labels]\n"," \n","# use the generator to generate n fake examples, with class labels\n","def generate_fake_samples(generator, latent_dim, n_samples):\n","\t# generate points in latent space\n","\tz_input, labels_input = generate_latent_points(latent_dim, n_samples)\n","\t# predict outputs\n","\timages = generator.predict([z_input, labels_input])\n","\t# create class labels\n","\ty = np.zeros((n_samples, 1))\n","\treturn [images, labels_input], y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8H8ykkSUEYYw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"cecb6b9e-a7e4-4da3-8d01-ed1fa0d00251"},"source":["def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100, n_batch=128):\n","    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n","    half_batch = int(n_batch / 2)\n","    \n","    train_lat_points,_ = generate_latent_points(100,9)\n","    train_labels =  np.arange(9)\n","    train_images = []\n","\n","    gan_loss = []\n","    disc_loss = []\n","\n","    # manually enumerate epochs\n","    for i in range(n_epochs):\n","        print(\"starting epoch\",i+1)\n","        # enumerate batches over the training set\n","        for j in range(bat_per_epo):\n","            # get randomly selected 'real' samples\n","            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n","            # update discriminator model weights\n","            d_loss1, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n","            # generate 'fake' examples\n","            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n","            # update discriminator model weights\n","            d_loss2, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n","            # prepare points in latent space as input for the generator\n","            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch)\n","            # create inverted labels for the fake samples\n","            y_gan = np.ones((n_batch, 1))\n","            # update the generator via the discriminator's error\n","            g_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n","            # summarize loss on this batch\n","            #print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' %(i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))\n","\n","            if j % 10 == 0:\n","                #save images\n","                imgs = g_model.predict([train_lat_points,train_labels])\n","                train_images.append(imgs)\n","\n","    # save the generator model\n","    g_model.save('cgan_generator.h5')\n","    \n","    return train_images\n","\n","disc = make_discriminator()\n","gen = define_generator()\n","\n","gan = define_gan(gen,disc)\n","\n","dataset = load_real_samples()\n","\n","train_images = train(gen,disc,gan,dataset,100,100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[<tf.Tensor 'input_94:0' shape=(?, 100) dtype=float32>, <tf.Tensor 'input_93:0' shape=(?, 1) dtype=float32>]\n","[<tf.Tensor 'input_92:0' shape=(?, 32, 32, 3) dtype=float32>, <tf.Tensor 'input_91:0' shape=(?, 1) dtype=float32>]\n","starting epoch 1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ojV4WeM2EiUn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":164},"outputId":"d1bf62a0-03e0-4a8f-854b-06bc10d8160a","executionInfo":{"status":"error","timestamp":1565535864652,"user_tz":-120,"elapsed":15976,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}}},"source":["train_images.shape"],"execution_count":65,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-65-764c035ef32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}]},{"cell_type":"code","metadata":{"id":"V0hatw6lEisf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"752a80c5-d256-44dd-d9c9-5888336c8a5f","executionInfo":{"status":"ok","timestamp":1565535896297,"user_tz":-120,"elapsed":16518,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}}},"source":["from matplotlib import animation\n","\n","ims = []\n","fig = plt.figure()\n","ax1 = fig.add_subplot()\n","ax2 = fig.add_subplot()\n","\"\"\"\n","ax = fig.add_axes([0,0,1,1], frameon=False, aspect=1)\n","ax.set_xticks([])\n","ax.set_yticks([])\n","\"\"\"\n","\n","for train_ims in train_images:\n","\n","    train_ims = (train_ims +1) /2.0\n","\n","    im_1 = plt.imshow(train_ims[5,:,:],vmin=0, vmax=1, animated=True) #add first image for test\n","    \n","    ims.append([im_1])\n","    #plt.pause(0.1) \n","\n","mp4_writer =  animation.writers['ffmpeg']\n","writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","\n","anim = animation.ArtistAnimation(fig,ims)\n","anim.save(\"test_gif_3.mp4\", writer= writer)"],"execution_count":66,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"EJ9c9kQiMyR3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"outputId":"a39239bc-8055-40fd-8c37-a0a5099623fe","executionInfo":{"status":"ok","timestamp":1565535995522,"user_tz":-120,"elapsed":26835,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}}},"source":["fig,axs = plt.subplots(3,3)\n","\n","print(axs.shape)\n","\n","for i in range(axs.shape[0]):\n","    for j in range(axs.shape[1]):\n","        \n","        axs[i,j].set_xticks([])\n","        axs[i,j].set_yticks([])\n","\n","images = []\n","\n","#iterate ove images\n","for imgs in train_images:\n","    ims = [] \n","    imgs = (imgs +1) /2.0\n","\n","    for i,img in enumerate(imgs):\n","        \n","        line = i%3\n","        col = int(i/3)\n","        ims.append(axs[line,col].imshow(img[:,:],vmin=0, vmax=1, animated=True))\n","    \n","    images.append(ims)\n","\n","mp4_writer =  animation.writers['ffmpeg']\n","writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","anim = animation.ArtistAnimation(fig,images)\n","anim.save(\"train_grid.mp4\", writer= writer)\n"],"execution_count":68,"outputs":[{"output_type":"stream","text":["(3, 3)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUEAAADuCAYAAACnM7W+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABBlJREFUeJzt3LFt40AURVHOwiXQsdl/LWQRju0e\nZlsgDQuSec+JJ/jAAy7AQBpzzgWg6t+zDwB4JhEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIG0tyuP\n13Wd27Y96JTXdxzH95zz/dl3/Da72vWOzu56KYLbti37vv/8qj9ujPH57Bsewa52vaOzu/ocBtJE\nEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0Eg\nTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQR\nBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBI\nE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgbc87zj8f4Wpbl83HnvLyPOef7s4/4bXa1602d2vVS\nBAHuxucwkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaW9XHq/rOrdte9Apr+84ju87\n/tDerna9o7O7Xorgtm3Lvu8/v+qPG2Pc8h857GrXOzq7q89hIE0EgTQRBNJEEEgTQSBNBIE0EQTS\nRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNB\nIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0\nEQTSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNJEEEgTQSBNBIE0EQTSRBBIE0EgTQSBNBEE0kQQ\nSBNBIE0EgbQx5zz/eIyvZVk+H3fOy/uYc74/+4jfZle73tSpXS9FEOBufA4DaSIIpIkgkCaCQJoI\nAmkiCKSJIJAmgkCaCAJpIgikvV15vK7r3LbtQae8vuM4vu/4G1O72vWOzu56KYLbti37vv/8qj9u\njHHLH6Pb1a53dHZXn8NAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggC\naSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJ\nIJAmgkCaCAJpIgikiSCQJoJAmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJA\nmggCaSIIpIkgkCaCQJoIAmkiCKSJIJAmgkCaCAJpIgikiSCQJoJAmggCaWPOef7xGF/Lsnw+7pyX\n9zHnfH/2Eb/Nrna9qVO7XoogwN34HAbSRBBIE0EgTQSBNBEE0kQQSBNBIE0EgTQRBNL+AyNQrLXD\nePj7AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 9 Axes>"]},"metadata":{"tags":[]}}]}]}